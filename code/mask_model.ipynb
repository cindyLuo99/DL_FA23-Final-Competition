{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"0nc76lCIutoP"},"outputs":[],"source":["!pip install torch==2.1.1 torchvision==0.16.1 torchaudio==2.1.1\n","!pip install torchmetrics"]},{"cell_type":"code","source":["from google.colab import drive\n","import os\n","drive.mount('/content/drive')"],"metadata":{"id":"XfpzgxkwwGze","executionInfo":{"status":"ok","timestamp":1702181010828,"user_tz":300,"elapsed":12628,"user":{"displayName":"Miles Pophal","userId":"10388167630996001461"}},"outputId":"39f9079d-addd-48e5-9b1f-2319633ab128","colab":{"base_uri":"https://localhost:8080/"}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["os.chdir(\"/content/drive/MyDrive/DL1008_Final Competition\")\n","data_folder = os.path.join(\"/content/drive/MyDrive/DL1008_Final Competition/Dataset_Student/train\")\n","print(data_folder)"],"metadata":{"id":"tzyufmll8Bxo"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Getting Standardization Parameters**"],"metadata":{"id":"zGfYBBop25Qk"}},{"cell_type":"code","source":["from unet_model import UNet"],"metadata":{"id":"J8bpyoSZANrH"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZcDnSCbQutoR"},"outputs":[],"source":["import sys\n","import torch\n","import numpy as np\n","import torchmetrics\n","from torch import nn, optim\n","from torch.utils.data import random_split,Dataset,DataLoader\n","from torchvision import tv_tensors\n","from torchvision.transforms import v2\n","import random\n","import torch.nn.functional as F\n","import PIL.Image"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"86knJk8-utoT"},"outputs":[],"source":["class Semantic_Segmentation_Dataset(Dataset):\n","\n","    def __init__(self, root_dir,input_transform = None,target_transform = None):\n","        self.root_dir= root_dir\n","        self.input_transform = input_transform\n","        self.target_transform = target_transform\n","\n","    def __len__(self):\n","        list_of_folders = [subdir for subdir in os.listdir(self.root_dir) if os.path.isdir(os.path.join(self.root_dir,subdir))]\n","        image_count = len(os.listdir(os.path.join(self.root_dir,list_of_folders[0]))) - 1\n","        total_folders = len(list_of_folders)\n","        return total_folders*image_count\n","\n","\n","    def transform(self, image,mask):\n","        if self.input_transform:\n","            image = self.input_transform(image)\n","        if self.target_transform:\n","            mask = self.target_transform(mask)\n","\n","        identical_transform = v2.Compose([\n","            v2.ToImage(),\n","            v2.RandomCrop(size=(160,240),padding=(40,60),padding_mode='edge'),\n","            v2.RandomHorizontalFlip(p=.75),\n","            v2.RandomVerticalFlip(p=.75),\n","            v2.RandomRotation(degrees=70)\n","        ])\n","\n","        new_img,new_msk = identical_transform((image,mask))\n","        return new_img,new_msk\n","\n","    def __getitem__(self, idx):\n","        base, idx = divmod(idx,22)\n","        folder_path = os.path.join(self.root_dir,f\"video_{base}\")\n","\n","        image,target = split_video_mask(folder_path,index=idx)\n","\n","\n","        if self.input_transform:\n","            image = self.input_transform(image)\n","        if self.target_transform:\n","            target = self.target_transform(target)\n","\n","        image,target = self.transform(image,target)\n","\n","        return image, target"]},{"cell_type":"markdown","metadata":{"id":"z801h2GUutoT"},"source":["**Loading Data**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2rkmn7HkutoW"},"outputs":[],"source":["def split_video_mask(video_folder,index):\n","    '''split_video_mask returns a tuple of tv_tensors\n","\n","    :param video_folder: directory to video folder\n","    :type video_folder: path\n","    :return: tuple of form (pytorch tensor, pytorch tensor) (size [3, 160, 240],[160, 240])\n","    '''\n","    directory = video_folder\n","\n","    img_path = os.path.join(video_folder,f\"image_{index}.png\")\n","    mask_path = os.path.join(video_folder,\"mask.npy\")\n","    assert os.path.isfile(img_path) and os.path.isfile(mask_path)\n","\n","    img = tv_tensors.Image(PIL.Image.open(os.path.join(video_folder,f\"image_{index}.png\"))).to(torch.get_default_dtype())\n","    all_masks = torch.from_numpy(np.load(os.path.join(directory,\"mask.npy\"))).to(torch.get_default_dtype())\n","    mask = tv_tensors.Image(all_masks[index])\n","\n","    return img, mask"]},{"cell_type":"code","source":["'''\n","sample_mean = torch.zeros(3)\n","sample_var = torch.zeros(3)\n","\n","for subdir in random.sample(os.listdir(data_folder),60): #about .03 off from true variance\n","    mask_list = split_video_mask(os.path.join(data_folder,subdir))\n","    for image,_ in mask_list:\n","      sample_mean += torch.mean(torch.Tensor(image),dim=[1,2])\n","      sample_var += torch.var(torch.Tensor(image),dim=[1,2])\n","\n","    sample_mean = sample_mean/len(mask_list)\n","    sample_var = sample_var/len(mask_list)\n","\n","sample_mean = sample_mean/60\n","sample_std = torch.sqrt(sample_var/60)\n","\n","mean,std =\n","#tensor([2.2536, 2.2524, 2.2107]), tensor([1.8866, 1.6751, 2.2449])\n","'''\n","sample_mean = torch.tensor([2.2536, 2.2524, 2.2107])\n","sample_std = torch.tensor([1.8866, 1.6751, 2.2449])"],"metadata":{"id":"WSwdbceO4_By"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(sample_mean,sample_std)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gTJYxAcu8UM_","executionInfo":{"status":"ok","timestamp":1702181044854,"user_tz":300,"elapsed":135,"user":{"displayName":"Miles Pophal","userId":"10388167630996001461"}},"outputId":"f0c4f590-371c-4653-c3f4-71eb2dfe16e3"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([2.2536, 2.2524, 2.2107]) tensor([1.8866, 1.6751, 2.2449])\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AYu1xx0JutoW"},"outputs":[],"source":["def load_data(video_folder):\n","\n","    input_transform = v2.Normalize(sample_mean,sample_std)\n","\n","\n","    ss_dataset = Semantic_Segmentation_Dataset(video_folder,\n","                                            input_transform=input_transform,\n","                                            target_transform=None\n","                                            )\n","\n","    generator = torch.Generator().manual_seed(10)\n","\n","    train_set, test_set = random_split(ss_dataset,[.7, .3],generator=generator)\n","\n","    return train_set, test_set"]},{"cell_type":"code","source":["def train_model(model, device,criterion,optimizer,train_loader,epoch,hyperparameters):\n","    model.train()\n","    print('Training')\n","    global best_val_jac\n","    batch_loss = 0\n","    jaccard = torchmetrics.JaccardIndex(task='multiclass',num_classes=49).to(device)\n","    height = 160\n","    width = 240\n","    correct_pixel = 0\n","\n","    for idx, batch in enumerate(train_loader):\n","        correct_pixel = 0\n","        data = batch[0].to(device)\n","        labels = batch[1].to(device).squeeze()\n","\n","        optimizer.zero_grad()\n","        predicted = model(data)\n","        loss = criterion(predicted,labels.long()).requires_grad_(True)\n","        batch_loss += loss.item()\n","        loss.backward()\n","        optimizer.step()\n","\n","        predicted = torch.argmax(predicted,dim=1)\n","\n","\n","        if idx % 100 == 0:\n","            print(f\"Epoch:{epoch}-{idx/(15400//hyperparameters['batch_size']):.2f}, loss: {loss.item():.2f}, jaccard: {100*jaccard(predicted,labels):.2f}%\")\n","\n","    batch_loss = batch_loss/idx\n","    torch.save({'epoch': epoch+1,\n","        'model_state_dict': model.state_dict(),\n","        'optimizer_state_dict': optimizer.state_dict(),\n","        'loss': batch_loss,\n","        'best_val_jac':best_val_jac,\n","        }, \"mask_model_stop.pth\")\n","\n"],"metadata":{"id":"aZsbyjhbOrlB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def validate_model(model, device,criterion,optimizer,val_loader,epoch,hyperparameters):\n","    # Validation loss\n","    print('Validating')\n","    global best_val_jac\n","    val_jac = 0\n","    total_pixels = 0\n","    correct_pixel = 0\n","    jaccard = torchmetrics.JaccardIndex(task='multiclass',num_classes=49).to(device)\n","    height = 160\n","    width = 240\n","    count = 0\n","\n","    for idx,batch in enumerate(val_loader):\n","        model.eval()\n","        with torch.no_grad():\n","            data = batch[0].to(device)\n","            labels = batch[1].to(device).squeeze()\n","\n","            predicted = model(data)\n","            #predicted = torch.argmax(predicted,dim=1)       #(Shape N,H,W)\n","\n","            #print(f\"Epoch: {epoch} Validation CEloss: {loss.item()}\")\n","            val_jac += 100 * jaccard(predicted,labels)\n","            predicted = torch.argmax(predicted,dim=1)\n","\n","\n","\n","        if idx % 100 == 0:\n","          count += 1\n","          print(f\"Step: {100*idx/206:.2f}\")\n","          for k in range(hyperparameters[\"batch_size\"]):\n","                for i in range(height):\n","                    for j in range(width):\n","                        if predicted[k][i][j].item() == labels[k][i][j].item():\n","                            correct_pixel += 1\n","    val_jac = val_jac/idx\n","    # Save the best model\n","\n","    if val_jac > best_val_jac:\n","        best_val_jac = val_jac\n","        torch.save({'epoch': epoch+1,\n","        'model_state_dict': model.state_dict(),\n","        'optimizer_state_dict': optimizer.state_dict(),\n","        'loss':batch_loss,\n","        'best_val_jac':best_val_jac,}, \"best_mask_model.pth\")\n","\n","    total_pixels = count * height * width *hyperparameters[\"batch_size\"]\n","    if epoch % 1 == 0:  #Running TR loss: {running_loss:.2f}\n","        print(f\"Epoch: {epoch}, Validation Accuracy (Jaccard): {val_jac:.2f}%, Pixel Accuracy: {100*correct_pixel/total_pixels:.2f}%\")"],"metadata":{"id":"_3gxA4dHO_OI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["    global best_val_jac\n","    global batch_loss\n","    best_val_jac = 0\n","    batch_loss = 0"],"metadata":{"id":"jufjPcEYUK7t"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"M9GzUwusutoW"},"outputs":[],"source":["def train_practice(hyperparameters, train_subset,val_subset):\n","    device = \"cpu\"\n","    if torch.cuda.is_available():\n","        device = \"cuda:0\"\n","\n","    net = UNet(n_channels=3,n_classes=49,bilinear=False).to(device)\n","    optimizer = optim.Adam(net.parameters(),lr=hyperparameters[\"lr\"])\n","\n","    if os.path.isfile(\"./mask_model_stop.pth\") and os.path.getsize(\"./mask_model_stop.pth\") > 0:\n","        checkpoint = torch.load(\"./mask_model_stop.pth\")\n","        net.load_state_dict(checkpoint['model_state_dict'])\n","        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n","        start_epoch = checkpoint['epoch']\n","        batch_loss = checkpoint['loss']\n","        best_val_jac = checkpoint['best_val_jac']\n","        print(f\"Resuming training from epoch: {start_epoch}\")\n","    else:\n","        best_val_jac = 0\n","        start_epoch = 0\n","        batch_loss = 0\n","\n","\n","    print(f\"Training on {device}\")\n","    net.to(device)\n","    jaccard = torchmetrics.JaccardIndex(task=\"multiclass\", num_classes=49).to(device).requires_grad_(True)\n","    #criterion = 1-jaccard.requires_grad_(True)\n","    criterion = nn.CrossEntropyLoss().to(device)\n","    print(\"++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\")\n","\n","\n","    trainloader = DataLoader(\n","        train_subset, batch_size=int(hyperparameters[\"batch_size\"]), shuffle=True, drop_last=True,num_workers=2\n","    )\n","    valloader = DataLoader(\n","        val_subset, batch_size=int(hyperparameters[\"batch_size\"]),num_workers=2,drop_last=True\n","    )\n","\n","    for epoch in range(start_epoch,hyperparameters[\"max_epochs\"]):\n","        train_model(net,device,criterion,optimizer,trainloader,epoch,hyperparameters)\n","        validate_model(net,device,criterion,optimizer,valloader,epoch,hyperparameters)"]},{"cell_type":"code","source":["train_subset, val_subset = load_data(data_folder)\n","#train_subset = torch.utils.data.Subset(train_subset,range(12000))\n","#val_subset = torch.utils.data.Subset(val_subset,range(300))\n","print(len(train_subset),len(val_subset))"],"metadata":{"id":"BsMMEhwgEfWl","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1702181063943,"user_tz":300,"elapsed":7979,"user":{"displayName":"Miles Pophal","userId":"10388167630996001461"}},"outputId":"040eccac-836b-47df-f32d-752946c8b71c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["15400 6600\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TUwSjW7rutoY","outputId":"2ea67aa0-ec84-4bff-d65d-2bfbf71e17ce","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"error","timestamp":1702188218362,"user_tz":300,"elapsed":7149316,"user":{"displayName":"Miles Pophal","userId":"10388167630996001461"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["tensor(87.1530, device='cuda:0') tensor(True, device='cuda:0')\n","Resuming training from epoch: 30\n","Training on cuda:0\n","++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n","Training\n","Epoch:30-0.00, loss: 0.01, jaccard: 86.22%\n","Epoch:30-0.21, loss: 0.01, jaccard: 86.28%\n","Epoch:30-0.42, loss: 0.01, jaccard: 84.68%\n","Epoch:30-0.62, loss: 0.01, jaccard: 87.05%\n","Epoch:30-0.83, loss: 0.01, jaccard: 86.83%\n","Validating\n","Step: 0.00\n","Step: 48.54\n","Step: 97.09\n","Epoch: 30, Validation Accuracy (Jaccard): 86.24%, Pixel Accuracy: 99.63%\n","Training\n","Epoch:31-0.00, loss: 0.01, jaccard: 89.66%\n","Epoch:31-0.21, loss: 0.01, jaccard: 85.19%\n","Epoch:31-0.42, loss: 0.01, jaccard: 82.60%\n","Epoch:31-0.62, loss: 0.01, jaccard: 86.72%\n","Epoch:31-0.83, loss: 0.01, jaccard: 86.66%\n","Validating\n","Step: 0.00\n","Step: 48.54\n","Step: 97.09\n","Epoch: 31, Validation Accuracy (Jaccard): 86.50%, Pixel Accuracy: 99.68%\n","Training\n","Epoch:32-0.00, loss: 0.01, jaccard: 89.47%\n","Epoch:32-0.21, loss: 0.01, jaccard: 85.52%\n","Epoch:32-0.42, loss: 0.01, jaccard: 94.28%\n","Epoch:32-0.62, loss: 0.01, jaccard: 87.75%\n","Epoch:32-0.83, loss: 0.01, jaccard: 88.20%\n","Validating\n","Step: 0.00\n","Step: 48.54\n","Step: 97.09\n","Epoch: 32, Validation Accuracy (Jaccard): 83.24%, Pixel Accuracy: 99.39%\n","Training\n","Epoch:33-0.00, loss: 0.01, jaccard: 93.14%\n","Epoch:33-0.21, loss: 0.01, jaccard: 86.94%\n","Epoch:33-0.42, loss: 0.01, jaccard: 85.79%\n","Epoch:33-0.62, loss: 0.01, jaccard: 89.51%\n","Epoch:33-0.83, loss: 0.01, jaccard: 87.41%\n","Validating\n","Step: 0.00\n","Step: 48.54\n","Step: 97.09\n","Epoch: 33, Validation Accuracy (Jaccard): 87.82%, Pixel Accuracy: 99.66%\n","Training\n","Epoch:34-0.00, loss: 0.01, jaccard: 85.93%\n","Epoch:34-0.21, loss: 0.01, jaccard: 91.26%\n","Epoch:34-0.42, loss: 0.01, jaccard: 89.41%\n","Epoch:34-0.62, loss: 0.01, jaccard: 84.32%\n","Epoch:34-0.83, loss: 0.01, jaccard: 83.13%\n","Validating\n","Step: 0.00\n","Step: 48.54\n","Step: 97.09\n","Epoch: 34, Validation Accuracy (Jaccard): 88.98%, Pixel Accuracy: 99.67%\n","Training\n","Epoch:35-0.00, loss: 0.01, jaccard: 91.14%\n","Epoch:35-0.21, loss: 0.01, jaccard: 88.23%\n","Epoch:35-0.42, loss: 0.01, jaccard: 88.17%\n","Epoch:35-0.62, loss: 0.01, jaccard: 87.64%\n","Epoch:35-0.83, loss: 0.01, jaccard: 90.59%\n","Validating\n","Step: 0.00\n","Step: 48.54\n","Step: 97.09\n","Epoch: 35, Validation Accuracy (Jaccard): 88.95%, Pixel Accuracy: 99.72%\n","Training\n","Epoch:36-0.00, loss: 0.01, jaccard: 90.33%\n","Epoch:36-0.21, loss: 0.01, jaccard: 90.36%\n","Epoch:36-0.42, loss: 0.01, jaccard: 84.36%\n","Epoch:36-0.62, loss: 0.00, jaccard: 91.99%\n","Epoch:36-0.83, loss: 0.01, jaccard: 91.27%\n","Validating\n","Step: 0.00\n","Step: 48.54\n","Step: 97.09\n","Epoch: 36, Validation Accuracy (Jaccard): 88.44%, Pixel Accuracy: 99.71%\n","Training\n","Epoch:37-0.00, loss: 0.01, jaccard: 91.94%\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-16-130d04297e8b>\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mmodel_below_threshold\u001b[0m\u001b[0;34m:\u001b[0m   \u001b[0;31m#train more\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m         \u001b[0mtrain_practice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhyperparameters\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_subset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mval_subset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m#model above threshold\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-14-8c1aba1a4c45>\u001b[0m in \u001b[0;36mtrain_practice\u001b[0;34m(hyperparameters, train_subset, val_subset)\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mhyperparameters\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"max_epochs\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m         \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrainloader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mhyperparameters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m         \u001b[0mvalidate_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvalloader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mhyperparameters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-11-82b4ad314055>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, device, criterion, optimizer, train_loader, epoch, hyperparameters)\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mpredicted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredicted\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequires_grad_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0mbatch_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["hyperparameters = {\"lr\":0.001,\"batch_size\":32,\"max_epochs\":50}\n","\n","model_exists = os.path.isfile(\"./best_mask_model.pth\") and os.path.getsize(\"./best_mask_model.pth\") > 0\n","threshold = 90\n","\n","if model_exists:\n","    #load model\n","    checkpoint = torch.load(\"./mask_model_stop.pth\")\n","\n","    #Loading Net\n","    net = UNet(n_channels=3,n_classes=49,bilinear=False)\n","    optimizer = optim.Adam(net.parameters(),lr=hyperparameters[\"lr\"])\n","    net.load_state_dict(checkpoint['model_state_dict'])\n","    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n","\n","    #Check best accuracy\n","    best_val_jac = checkpoint['best_val_jac']\n","    model_below_threshold = best_val_jac < threshold\n","    print(best_val_jac,model_below_threshold)\n","\n","    if model_below_threshold:   #train more\n","        train_practice(hyperparameters,train_subset,val_subset)\n","    else: #model above threshold\n","        pass\n","else: #no model\n","    train_practice(hyperparameters,train_subset,val_subset)\n","\n","#Model is Trained\n","net = UNet(n_channels=3,n_classes=49,bilinear=False)\n","checkpoint = torch.load(\"./best_mask_model.pth\")\n","#net.classifier._modules['4'] = torch.nn.Conv2d(256, 49, kernel_size=(1, 1), stride=(1, 1))\n","optimizer = optim.Adam(net.parameters(),lr=hyperparameters[\"lr\"])\n","net.load_state_dict(checkpoint['model_state_dict'])\n","optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n","best_val_jac = checkpoint[\"best_val_jac\"]\n","print(f\"Final validation accuracy (Jaccard): {best_val_jac:.2f}%\")"]},{"cell_type":"code","source":["checkpoint = torch.load(\"./89valjac.pth\")\n","print(checkpoint['best_val_jac'].item())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gwkOCw7x_oLM","executionInfo":{"status":"ok","timestamp":1702188579845,"user_tz":300,"elapsed":4949,"user":{"displayName":"Miles Pophal","userId":"10388167630996001461"}},"outputId":"5483417b-1010-4bbc-cf5e-3bd757032c0f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["88.97635650634766\n"]}]},{"cell_type":"code","source":["import gc"],"metadata":{"id":"vM0Bs6orBLnn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["torch.cuda.empty_cache()\n","gc.collect()"],"metadata":{"id":"cBJspAzWFuOI","executionInfo":{"status":"ok","timestamp":1702188227493,"user_tz":300,"elapsed":252,"user":{"displayName":"Miles Pophal","userId":"10388167630996001461"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"0895f7b7-346a-4a9b-f208-c3d35aae889f"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["2853"]},"metadata":{},"execution_count":20}]}],"metadata":{"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"},"colab":{"provenance":[{"file_id":"https://github.com/cindyLuo99/DL_FA23-Final-Competition/blob/main/practice_model.ipynb","timestamp":1701726198904}],"gpuType":"T4"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}
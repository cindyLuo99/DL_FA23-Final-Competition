{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.io import read_image\n",
    "from torchvision.transforms import Compose, Resize, ToTensor, Normalize\n",
    "import torchmetrics\n",
    "\n",
    "device = torch.device('cuda:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "shared_path =\"/scratch/kl3108/DS1008/finalCompetition/\" # change to your own path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VideoSegDataset(Dataset):\n",
    "    def __init__(self, root_dir):\n",
    "        self.root_dir = root_dir\n",
    "        self.video_folders = [os.path.join(root_dir, f) for f in os.listdir(root_dir) if os.path.isdir(os.path.join(root_dir, f))]\n",
    "        self.video_folders.sort()\n",
    "        #self.video_folders = self.video_folders # slide into different part\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.video_folders)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        video_folder = self.video_folders[idx]\n",
    "\n",
    "        # Load segmentation masks\n",
    "        mask_path = os.path.join(video_folder, 'mask.pt')\n",
    "        masks = torch.load(mask_path).long()  # Shape [11, 160, 240]\n",
    "        \n",
    "        # Split into input and target masks\n",
    "        input_masks = masks\n",
    "\n",
    "        # Add channel dimension\n",
    "        # One-hot encode\n",
    "        input_masks = torch.nn.functional.one_hot(input_masks, num_classes=49).permute(0, 3, 1, 2).float() # Shape [22, 49, 160, 240]\n",
    "\n",
    "        return input_masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_dataset = VideoSegDataset(root_dir=shared_path+'hidden/') #from 1000-1999 sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "batchsize = 16\n",
    "hidden_loader = DataLoader(hidden_dataset, batch_size = batchsize, shuffle=False, pin_memory=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicConv2d(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride, padding, transpose=False, act_norm=False):\n",
    "        super(BasicConv2d, self).__init__()\n",
    "        self.act_norm=act_norm\n",
    "        if not transpose:\n",
    "            self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size, stride=stride, padding=padding)\n",
    "        else:\n",
    "            self.conv = nn.ConvTranspose2d(in_channels, out_channels, kernel_size=kernel_size, stride=stride, padding=padding,output_padding=stride //2 )\n",
    "        self.norm = nn.GroupNorm(2, out_channels)\n",
    "        self.act = nn.LeakyReLU(0.2, inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = self.conv(x)\n",
    "        if self.act_norm:\n",
    "            y = self.act(self.norm(y))\n",
    "        return y\n",
    "\n",
    "class ConvSC(nn.Module):\n",
    "    def __init__(self, C_in, C_out, stride, transpose=False, act_norm=True):\n",
    "        super(ConvSC, self).__init__()\n",
    "        if stride == 1:\n",
    "            transpose = False\n",
    "        self.conv = BasicConv2d(C_in, C_out, kernel_size=3, stride=stride,\n",
    "                                padding=1, transpose=transpose, act_norm=act_norm)\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = self.conv(x)\n",
    "        return y\n",
    "\n",
    "\n",
    "class GroupConv2d(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride, padding, groups, act_norm=False):\n",
    "        super(GroupConv2d, self).__init__()\n",
    "        self.act_norm = act_norm\n",
    "        if in_channels % groups != 0:\n",
    "            groups = 1\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size, stride=stride, padding=padding,groups=groups)\n",
    "        self.norm = nn.GroupNorm(groups,out_channels)\n",
    "        self.activate = nn.LeakyReLU(0.2, inplace=True)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        y = self.conv(x)\n",
    "        if self.act_norm:\n",
    "            y = self.activate(self.norm(y))\n",
    "        return y\n",
    "\n",
    "\n",
    "class Inception(nn.Module):\n",
    "    def __init__(self, C_in, C_hid, C_out, incep_ker=[3,5,7,11], groups=8):        \n",
    "        super(Inception, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(C_in, C_hid, kernel_size=1, stride=1, padding=0)\n",
    "        layers = []\n",
    "        for ker in incep_ker:\n",
    "            layers.append(GroupConv2d(C_hid, C_out, kernel_size=ker, stride=1, padding=ker//2, groups=groups, act_norm=True))\n",
    "        self.layers = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        y = 0\n",
    "        for layer in self.layers:\n",
    "            y += layer(x)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stride_generator(N, reverse=False):\n",
    "    '''\n",
    "    ### 1. `stride_generator` Function\n",
    "    This function generates a list of stride values for convolutional layers. \n",
    "    A stride determines how far a convolutional filter moves across the input. In this function:\n",
    "    - `strides = [1, 2]*10` creates a list of alternating 1s and 2s.\n",
    "    - `N` specifies how many elements from this list to use.\n",
    "    - If `reverse` is `True`, the list is reversed; otherwise, it's used as is.\n",
    "    '''\n",
    "    strides = [1, 2]*10\n",
    "    if reverse: return list(reversed(strides[:N]))\n",
    "    else: return strides[:N] # N should be less than 20\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    '''\n",
    "    ### 2. `Encoder` Class\n",
    "    This class defines an encoder module, typically used to downsample and extract features from the input.\n",
    "    - `C_in` and `C_hid` are the number of input and hidden channels, respectively.\n",
    "    - `N_S` is the number of strides, used to generate stride values.\n",
    "    - `self.enc` is a sequential container of convolutional layers (`ConvSC`), where the first layer's stride is `strides[0]` and subsequent layers' strides are determined by the remaining elements in `strides`.\n",
    "    '''\n",
    "    def __init__(self,C_in, C_hid, N_S):\n",
    "        super(Encoder,self).__init__()\n",
    "        strides = stride_generator(N_S)\n",
    "        self.enc = nn.Sequential(\n",
    "            ConvSC(C_in, C_hid, stride=strides[0]),\n",
    "            *[ConvSC(C_hid, C_hid, stride=s) for s in strides[1:]]\n",
    "        )\n",
    "    \n",
    "    def forward(self,x):# B*4, 3, 128, 128\n",
    "        enc1 = self.enc[0](x)\n",
    "        latent = enc1\n",
    "        for i in range(1,len(self.enc)):\n",
    "            latent = self.enc[i](latent)\n",
    "        return latent,enc1\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    '''\n",
    "    ### 3. `Decoder` Class\n",
    "    This class defines a decoder module, which typically upsamples the feature maps to reconstruct or generate output.\n",
    "    - It uses the same stride values as the encoder but in reverse order. This symmetry is common in encoder-decoder architectures.\n",
    "    - `ConvSC` with `transpose=True` indicates transposed convolution, often used for upsampling.\n",
    "    - The final layer concatenates its input with the feature map `enc1` from the encoder, which is a form of skip connection, often used to help the network better learn fine-grained details.\n",
    "    '''\n",
    "    def __init__(self,C_hid, C_out, N_S):\n",
    "        super(Decoder,self).__init__()\n",
    "        strides = stride_generator(N_S, reverse=True)\n",
    "        self.dec = nn.Sequential(\n",
    "            *[ConvSC(C_hid, C_hid, stride=s, transpose=True) for s in strides[:-1]],\n",
    "            ConvSC(2*C_hid, C_hid, stride=strides[-1], transpose=True)\n",
    "        )\n",
    "        self.readout = nn.Conv2d(C_hid, C_out, 1)\n",
    "    \n",
    "    def forward(self, hid, enc1=None):\n",
    "        for i in range(0,len(self.dec)-1):\n",
    "            hid = self.dec[i](hid)\n",
    "        Y = self.dec[-1](torch.cat([hid, enc1], dim=1))\n",
    "        Y = self.readout(Y)\n",
    "        return Y\n",
    "\n",
    "class Mid_Xnet(nn.Module):\n",
    "    '''\n",
    "    ### 4. `Mid_Xnet` Class\n",
    "    This class appears to be a middle processing network with multiple `Inception` modules.\n",
    "    - It alternates between encoding and decoding Inception modules, with skip connections from encoder layers to corresponding decoder layers.\n",
    "    '''\n",
    "    def __init__(self, channel_in, channel_hid, N_T, incep_ker = [3,5,7,11], groups=8):\n",
    "        super(Mid_Xnet, self).__init__()\n",
    "\n",
    "        self.N_T = N_T\n",
    "        enc_layers = [Inception(channel_in, channel_hid//2, channel_hid, incep_ker= incep_ker, groups=groups)]\n",
    "        for i in range(1, N_T-1):\n",
    "            enc_layers.append(Inception(channel_hid, channel_hid//2, channel_hid, incep_ker= incep_ker, groups=groups))\n",
    "        enc_layers.append(Inception(channel_hid, channel_hid//2, channel_hid, incep_ker= incep_ker, groups=groups))\n",
    "\n",
    "        dec_layers = [Inception(channel_hid, channel_hid//2, channel_hid, incep_ker= incep_ker, groups=groups)]\n",
    "        for i in range(1, N_T-1):\n",
    "            dec_layers.append(Inception(2*channel_hid, channel_hid//2, channel_hid, incep_ker= incep_ker, groups=groups))\n",
    "        dec_layers.append(Inception(2*channel_hid, channel_hid//2, channel_in, incep_ker= incep_ker, groups=groups))\n",
    "\n",
    "        self.enc = nn.Sequential(*enc_layers)\n",
    "        self.dec = nn.Sequential(*dec_layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C, H, W = x.shape\n",
    "        x = x.reshape(B, T*C, H, W)\n",
    "\n",
    "        # encoder\n",
    "        skips = []\n",
    "        z = x\n",
    "        for i in range(self.N_T):\n",
    "            z = self.enc[i](z)\n",
    "            if i < self.N_T - 1:\n",
    "                skips.append(z)\n",
    "\n",
    "        # decoder\n",
    "        z = self.dec[0](z)\n",
    "        for i in range(1, self.N_T):\n",
    "            z = self.dec[i](torch.cat([z, skips[-i]], dim=1))\n",
    "\n",
    "        y = z.reshape(B, T, C, H, W)\n",
    "        return y\n",
    "\n",
    "\n",
    "class SimVP(nn.Module):\n",
    "    '''\n",
    "    ### 5. `SimVP` Class\n",
    "    This is the main model class combining the Encoder, Mid_Xnet, and Decoder.\n",
    "    - `shape_in` describes the shape of the input tensor.\n",
    "    - The model first reshapes the input and passes it through the encoder.\n",
    "    - The encoded output is reshaped again and passed through `Mid_Xnet`.\n",
    "    - Finally, the output of `Mid_Xnet` is reshaped and decoded to produce the final output.\n",
    "    '''\n",
    "    def __init__(self, shape_in, hid_S=16, hid_T=256, N_S=4, N_T=8, incep_ker=[3,5,7,11], groups=8):\n",
    "        super(SimVP, self).__init__()\n",
    "        T, C, H, W = shape_in\n",
    "        self.enc = Encoder(C, hid_S, N_S)\n",
    "        self.hid = Mid_Xnet(T*hid_S, hid_T, N_T, incep_ker, groups)\n",
    "        self.dec = Decoder(hid_S, C, N_S)\n",
    "\n",
    "\n",
    "    def forward(self, x_raw):\n",
    "        B, T, C, H, W = x_raw.shape\n",
    "        x = x_raw.view(B*T, C, H, W)\n",
    "\n",
    "        embed, skip = self.enc(x)\n",
    "        _, C_, H_, W_ = embed.shape\n",
    "\n",
    "        z = embed.view(B, T, C_, H_, W_)\n",
    "        hid = self.hid(z)\n",
    "        hid = hid.reshape(B*T, C_, H_, W_)\n",
    "\n",
    "        Y = self.dec(hid, skip)\n",
    "        Y = Y.reshape(B, T, C, H, W)\n",
    "        return Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = SimVP(tuple([11,49,160,240]), 64, 256, 4, 8, [3,5,7,11], 7).to(device)\n",
    "model.load_state_dict(torch.load(shared_path+'Sim_VP_best_model_segs_new_loss_9.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 125/125 [09:58<00:00,  4.79s/it]\n"
     ]
    }
   ],
   "source": [
    "hidden_pbar = tqdm(hidden_loader)\n",
    "pred_result = []\n",
    "for i, data in enumerate(hidden_pbar):\n",
    "    data = data.to(device)\n",
    "    with torch.no_grad():  # No need to track gradients during validation\n",
    "        pred_last = model(data)[:,-1,:,:,:].squeeze()    #batch,49,160,240\n",
    "    pred_result.append(torch.argmax(pred_last,dim=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2000, 160, 240])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = torch.cat(pred_result)\n",
    "result.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(result, shared_path + \"final_leaderboard_team_36.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(result, \"final_leaderboard_team_36.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
